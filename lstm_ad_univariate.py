# -*- coding: utf-8 -*-
"""LSTM_AD_Univariate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YOkPG3AvB0Z-W7hcCOOul6H8SFdZh8_Z

Check mydrive content:

Which GPU Am I Using?
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras

from keras.models import Sequential
from keras.layers import LSTM, Input, Dropout
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import TimeDistributed

from pandas.plotting import register_matplotlib_converters

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from keras.models import Model

from matplotlib import pyplot as plt
from matplotlib import rc
from matplotlib import rcParams

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

register_matplotlib_converters()#helps plotting the dates easier!
sns.set(style='darkgrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 10, 8

df_org = pd.read_csv('/content/drive/My Drive/Colab dataset/DOGE_USD_historical.csv')
print('Shape:', df_org.shape)
print('Header:')
#print(df_org.head())
df_org

df_for_plot = df_org.tail(1000) #This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows.
df_for_plot.plot.line()
plt.title('Features Overview')
plt.xticks(rotation=25)
plt.legend();

# check for nul, nan, 0
df_org.isnull().any()

df_org = df_org.dropna()
df_org.isnull().any()

# creating data frame and indexing
df = df_org[['Date', 'Volume']]
df['Date'] = pd.to_datetime(df['Date'])

# plotting date against volume
df.plot(x='Date', y='Volume')
plt.xlabel('Date')
plt.ylabel('Volume per transaction')
plt.title('Time Series Volume Over time')

"""# Preprcessing
## Splitting
90% for training
10% for testing
"""

# training = 90% , test = 10% of len(df1)
train_size = int(len(df) * 0.90)
test_size = len(df) - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]
print(train.shape, test.shape)

"""## Scaling
LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used as activation functions. rescale the data to the range of 0-to-1, also called normalizing.
"""

train

fig, (ax1) = plt.subplots()#ncols=1, figsize=(8, 5))
ax1.set_title('Before Scaling')
sns.kdeplot(df['Volume'], ax=ax1)

# Standardize features by removing the MEAN and scaling to UNIT VARIANCE!
scaler = StandardScaler() 
scaler = scaler.fit(train[['Volume']])

train['Volume'] = scaler.transform(train[['Volume']])
test['Volume'] = scaler.transform(test[['Volume']])

train['Volume']

fig, (ax1) = plt.subplots()#ncols=1, figsize=(8, 5))
ax1.set_title('After Scaling')
sns.kdeplot(train['Volume'], ax=ax1)

print(train.shape)
print(test.shape)

"""### Helper Function to Creat Proper Shape Data
hen we reshape our data into a format suitable for input into an LSTM network. LSTM cells expect a 3 dimensional tensor of the form [data samples, time steps, features]. Here, each sample input into the LSTM network represents one step in time and contains 1 feature — the volume of coins!.
"""

sequence = 30  # Number of time steps to look back 
# Xs are our sequence = 30, Y is what comes after the sequence = 1

def to_sequences(x, y, sequence=1):
    x_vals = []
    y_vals = []
    for i in range(len(x)-sequence):
        #print(i)
        x_vals.append(x.iloc[i:(i+sequence)].values)
        y_vals.append(y.iloc[i+sequence])   
    return np.array(x_vals), np.array(y_vals)

trainX, trainY = to_sequences(train[['Volume']], train['Volume'], sequence)
testX, testY = to_sequences(test[['Volume']], test['Volume'], sequence)

print(trainX.shape)
print(testX.shape)
# first 30 samples are reserved!

"""# LSTM Autoencoder
Autoencoder should take a sequence as input and outputs a sequence of the same shape.
"""

#LSTM Autoencoder Configs 04
model = keras.Sequential()
model.add(LSTM(32, input_shape=(trainX.shape[1], trainX.shape[2])))
model.add(Dropout(rate=0.2)) # applying regularization To reduce overfitting

model.add(RepeatVector(trainX.shape[1]))# to shape input for TimeDistributed

model.add(LSTM(32, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))

model.add(TimeDistributed(Dense(trainX.shape[2]))) #applies a layer to every temporal slice of an input to
# creates a vector with a length of the number of outputs from the previous layer

model.compile(optimizer='adam', loss='mae')
# model.summary()


# fitting the model
history = model.fit(
    trainX, trainY,
    epochs=10,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

"""The RepeatVector layer simply repeats the input n times.
return_sequences=True in LSTM layer makes it return the sequence.
TimeDistributed layer creates a vector with a length of the number of outputs from the previous layer.

Note: ‘None’ is the batch number. In our case batch_size = 64!
"""

model.summary()

# plotting training loss and validation loss
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()
plt.title('Loss Error')

# Plotting the train prediction error
# Anologous to Z-test!
train_pred = model.predict(trainX)
train_MAE = np.mean(np.abs(train_pred - trainX), axis=1)
sns.distplot(train_MAE, bins=50, kde=True, axlabel='Train: Mean Absolute Error');

plt.ylabel('Observed Error')
plt.title('Training Erorr')

# Recunstruction error = MeanAbsoluteError in this case!
test_pred = model.predict(testX) # y_hat
test_MAE = np.mean(np.abs(test_pred - testX), axis=1)
sns.distplot(test_MAE, bins=50, kde=True, axlabel='Test: Mean Absolute Error');


plt.ylabel('Observed Error')
plt.title('Testing Error')

max_train_MAE = 50  # Threshold/max of reconstruction error!

#Capture all details in a DataFrame for easy plotting
anomaly_df = pd.DataFrame(test[sequence:])
anomaly_df['test_MAE'] = test_MAE
anomaly_df['max_train_MAE'] = max_train_MAE
anomaly_df['Anomaly'] = anomaly_df['test_MAE'] > anomaly_df['max_train_MAE']
anomaly_df['Volume'] = test[sequence:]['Volume']

anomaly_df

#Plot test_MAE vs max_train_MAE
sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['test_MAE'], label='Mean Absolute Error')
sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['max_train_MAE'], label='Threshold')

plt.xlabel('Date')
plt.ylabel('Reconstruction Error')
plt.title('Prediction Error = predicted - observed')
plt.xticks(rotation=25)
plt.legend();

anomalies = anomaly_df.loc[anomaly_df['Anomaly'] == True]
anomalies.head(10)

#Plot anomalies
sns.lineplot(x=anomaly_df['Date'], y=scaler.inverse_transform(anomaly_df['Volume']), label='Volume')
sns.scatterplot(x=anomalies['Date'], y=scaler.inverse_transform(anomalies['Volume']), color='r', s=100, label='anomaly')

plt.xlabel('Date')
plt.ylabel('Volume*e10')
plt.title('Detected Anomalies')
plt.xticks(rotation=25)
plt.legend();

